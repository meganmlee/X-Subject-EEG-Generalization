{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":24129,"status":"ok","timestamp":1761267620322,"user":{"displayName":"Megan Lee","userId":"00067592574554079062"},"user_tz":240},"id":"jp806ScUBaOk","outputId":"ccd3d8a6-3d52-489b-90d1-8b808e4ce152"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: mne in /usr/local/lib/python3.12/dist-packages (1.10.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne) (4.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne) (3.1.6)\n","Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne) (0.4)\n","Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.12/dist-packages (from mne) (3.10.0)\n","Requirement already satisfied: numpy<3,>=1.25 in /usr/local/lib/python3.12/dist-packages (from mne) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne) (25.0)\n","Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne) (1.8.2)\n","Requirement already satisfied: scipy>=1.11 in /usr/local/lib/python3.12/dist-packages (from mne) (1.16.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne) (4.67.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne) (2.9.0.post0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (4.5.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne) (2.32.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne) (3.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7->mne) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.10.5)\n","Requirement already satisfied: mne-connectivity in /usr/local/lib/python3.12/dist-packages (0.7.0)\n","Requirement already satisfied: mne>=1.6 in /usr/local/lib/python3.12/dist-packages (from mne-connectivity) (1.10.2)\n","Requirement already satisfied: netCDF4>=1.6.5 in /usr/local/lib/python3.12/dist-packages (from mne-connectivity) (1.7.3)\n","Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from mne-connectivity) (2.0.2)\n","Requirement already satisfied: pandas>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from mne-connectivity) (2.2.2)\n","Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mne-connectivity) (1.16.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne-connectivity) (4.67.1)\n","Requirement already satisfied: xarray>=2023.11.0 in /usr/local/lib/python3.12/dist-packages (from mne-connectivity) (2025.10.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne>=1.6->mne-connectivity) (4.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne>=1.6->mne-connectivity) (3.1.6)\n","Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne>=1.6->mne-connectivity) (0.4)\n","Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.12/dist-packages (from mne>=1.6->mne-connectivity) (3.10.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne>=1.6->mne-connectivity) (25.0)\n","Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne>=1.6->mne-connectivity) (1.8.2)\n","Requirement already satisfied: cftime in /usr/local/lib/python3.12/dist-packages (from netCDF4>=1.6.5->mne-connectivity) (1.6.5)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netCDF4>=1.6.5->mne-connectivity) (2025.10.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.2->mne-connectivity) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.2->mne-connectivity) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.2->mne-connectivity) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.6->mne-connectivity) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.6->mne-connectivity) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.6->mne-connectivity) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.6->mne-connectivity) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.6->mne-connectivity) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.6->mne-connectivity) (3.2.5)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne>=1.6->mne-connectivity) (4.5.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne>=1.6->mne-connectivity) (2.32.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.2->mne-connectivity) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne>=1.6->mne-connectivity) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.6->mne-connectivity) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.6->mne-connectivity) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.6->mne-connectivity) (2.5.0)\n","Requirement already satisfied: ema_pytorch in /usr/local/lib/python3.12/dist-packages (0.7.7)\n","Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from ema_pytorch) (2.8.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->ema_pytorch) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->ema_pytorch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->ema_pytorch) (3.0.3)\n"]}],"source":["!pip install mne\n","!pip install mne-connectivity\n","!pip install ema_pytorch"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":10660,"status":"ok","timestamp":1761267647581,"user":{"displayName":"Megan Lee","userId":"00067592574554079062"},"user_tz":240},"id":"pV3ufSADRHh3"},"outputs":[],"source":["import os\n","import pandas as pd\n","import mne\n","import numpy as np\n","from pathlib import Path\n","import torch\n","from collections import defaultdict\n","\n","# Load one subject's data by session\n","def load_data_by_session(root_dir, subject_id, session_idx_list):\n","    data = np.load(os.path.join(root_dir, f\"S{subject_id}_chars.npy\"))\n","    data = data[:, session_idx_list]\n","    X = data.reshape(-1, 64, 250)\n","    Y = np.repeat(np.arange(26), len(session_idx_list))\n","    return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.long)\n","\n","# --------- MI ---------\n","def MI_load_data_by_session(root_dir, subject_id, session_folders, label_dir):\n","    \"\"\"\n","    root_dir/\n","      first_session/\n","        A01T_cleaned.fif … A09T_cleaned.fif\n","      second_session/\n","        A01E_cleaned.fif … A09E_cleaned.fif\n","\n","    session_folders: list of folder names, e.g. [\"first_session\"] or [\"second_session\"]\n","    \"\"\"\n","    X_list, Y_list = [], []\n","\n","    prefix = \"T\"\n","    fname = f\"A{subject_id:02d}{prefix}.fif\"\n","    fpath = os.path.join(root_dir, \"first_session\", fname)\n","    raw = mne.io.read_raw_fif(fpath, preload=True, verbose=False)\n","\n","    # MI cue as '769'~'772'，mapping as 0–3 labels\n","    events, event_id = mne.events_from_annotations(raw, verbose=False)\n","    motor_keys = ['769', '770', '771', '772']\n","    motor_event_id = {k: v for k, v in event_id.items() if k in motor_keys}\n","    if len(motor_event_id) < 4:\n","        raise ValueError(f\"{fname} missing MI cues. Found: {event_id}\")\n","    events = np.array([e for e in events if e[2] in motor_event_id.values()])\n","    label_map = {\n","        motor_event_id['769']: 0,\n","        motor_event_id['770']: 1,\n","        motor_event_id['771']: 2,\n","        motor_event_id['772']: 3,\n","    }\n","    labels = np.array([label_map[e[-1]] for e in events])\n","\n","    epochs = mne.Epochs(\n","        raw, events,\n","        tmin=0.0,\n","        tmax=4.0,\n","        baseline=None,\n","        preload=True,\n","        verbose=False,\n","        event_repeated=\"drop\"\n","    )\n","    data = epochs.get_data()\n","    X_list.append(torch.from_numpy(data).float())\n","    Y_list.append(torch.from_numpy(labels).long())\n","\n","    X = torch.cat(X_list, dim=0)\n","    Y = torch.cat(Y_list, dim=0)\n","    return X, Y\n","\n","# --------- P300 ---------\n","def P300_load_subject_data(subject_id, root_dir):\n","    folder = os.path.join(root_dir, f\"subject_{subject_id:02d}\")\n","    X = np.load(os.path.join(folder, \"X.npy\"))                # shape: (n_trials, C, T)\n","    Y = np.load(os.path.join(folder, \"y.npy\"))                # shape: (n_trials,)\n","    Y = np.array([1 if label == 'Target' else 0 for label in Y])\n","\n","    meta = pd.read_csv(os.path.join(folder, \"metadata.csv\"))  # contains at least 'session'\n","\n","    trials_per_repetition = 12\n","    reps_per_level = 8\n","    trials_per_level = reps_per_level * trials_per_repetition  # 96\n","    levels_per_session = 9\n","\n","    level_list = []\n","    repetition_list = []\n","\n","    for sess in sorted(meta[\"session\"].unique()):\n","        session_idxs = meta.index[meta[\"session\"] == sess].tolist()\n","        for i, idx in enumerate(session_idxs):\n","            rep = i // trials_per_repetition\n","            level = rep // reps_per_level\n","            repetition = rep % reps_per_level\n","            level_list.append(level)\n","            repetition_list.append(repetition)\n","\n","    meta[\"level\"] = level_list\n","    meta[\"repetition\"] = repetition_list\n","\n","    return {\n","        \"X\": X,\n","        \"Y\": Y,\n","        \"session\": meta[\"session\"].tolist(),\n","        \"level\": meta[\"level\"].tolist(),\n","        \"repetition\": meta[\"repetition\"].tolist()\n","    }\n","\n","# --------- Imagined_speech ---------\n","def ImaginedSpeech_load_subject_data(subject_id, root_dir):\n","    x_path = os.path.join(root_dir, f\"epochs_{subject_id}_notched.npy\")\n","    y_path = os.path.join(root_dir, f\"labels_{subject_id}.npy\")\n","\n","    X = np.load(x_path)  # shape: (n_trials, C, T)\n","    Y_raw = np.load(y_path, allow_pickle=True)  # shape: (n_trials,), string labels\n","\n","    Y_raw = Y_raw.flatten()\n","\n","    # Map labels to level indices\n","    label_set = sorted(set(Y_raw.tolist()))\n","    label2idx = {label: i for i, label in enumerate(label_set)}  # consistent across subjects\n","\n","    level = [label2idx[label] for label in Y_raw]\n","\n","    # Build repetition index for each stimulus\n","    counter = defaultdict(int)\n","    repetition = []\n","    for label in Y_raw:\n","        repetition.append(counter[label])\n","        counter[label] += 1\n","\n","    # All trials are from a single session\n","    session = [0] * len(Y_raw)\n","\n","    return {\n","        \"X\": X,\n","        \"Y\": np.array(level),\n","        \"session\": session,\n","        \"level\": level,\n","        \"repetition\": repetition\n","    }"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":477},"executionInfo":{"elapsed":6848,"status":"error","timestamp":1761267656600,"user":{"displayName":"Megan Lee","userId":"00067592574554079062"},"user_tz":240},"id":"NHNPtYdGBhSi","outputId":"47c19648-a3ae-4ce9-e5ce-2f0ed72f6e33"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","Cross-Trial Consistency Analysis - P300 Task\n","================================================================================\n","\n","Loading ALL P300 data from: /content/drive/MyDrive/IDL/IDL Project Team 5 F25/dataset/p300/bi2015a/cleaned_data\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1617989905.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m     \u001b[0msubject_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_all_subjects_for_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{'='*40}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1617989905.py\u001b[0m in \u001b[0;36mload_all_subjects_for_task\u001b[0;34m(task, root_dir, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_all_SSVEP_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'P300'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_all_P300_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Imagined_speech'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_all_ImaginedSpeech_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1617989905.py\u001b[0m in \u001b[0;36mload_all_P300_data\u001b[0;34m(root_dir)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0msubject_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded Subject {subject_id}: {len(Y)} trials\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\"\"\"\n","Cross-Trial Analysis\n","\"\"\"\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from pathlib import Path\n","from collections import defaultdict\n","\n","# ============================================================================\n","# CONFIGURATION - Set your task here\n","# ============================================================================\n","\n","TASK = 'P300'  # Change to: 'MI', 'SSVEP', 'P300', 'Imagined_speech'\n","\n","# Data paths (update these)\n","DATA_PATHS = {\n","    'MI': '/content/drive/MyDrive/IDL/IDL Project Team 5 F25/dataset/MI/cleaned_data',\n","    'SSVEP': '/content/drive/MyDrive/IDL/IDL Project Team 5 F25/dataset/ssvep/chars',\n","    'P300': '/content/drive/MyDrive/IDL/IDL Project Team 5 F25/dataset/p300/bi2015a/cleaned_data',\n","    'Imagined_speech': '/content/drive/MyDrive/IDL/IDL Project Team 5 F25/dataset/speech_imagined/KARA_ONE/epochs/notched'\n","}\n","\n","OUTPUT_DIR = Path(f'/content/drive/MyDrive/IDL/IDL Project Team 5 F25/data analysis/{TASK}/cross_trial_{TASK}')\n","OUTPUT_DIR.mkdir(exist_ok=True)\n","\n","# Task-specific parameters\n","TASK_CONFIG = {\n","    'MI': {'sfreq': 250, 'n_classes': 4, 'use_alignment': True},\n","    'SSVEP': {'sfreq': 250, 'n_classes': 26, 'use_alignment': False},\n","    'P300': {'sfreq': 256, 'n_classes': 2, 'use_alignment': False},\n","    'Imagined_speech': {'sfreq': 128, 'n_classes': 11, 'use_alignment': False}\n","}\n","\n","freq_bands = {'Theta': (4, 8), 'Alpha': (8, 13), 'Beta': (13, 30), 'Gamma': (30, 40)}\n","\n","# ============================================================================\n","# NEW: DATA LOADING from dataset.py - Load ALL data for analysis\n","# ============================================================================\n","\n","def load_all_subjects_for_task(task, root_dir, **kwargs):\n","    \"\"\"\n","    Load ALL data for all subjects (not split) - for data analysis purposes.\n","    Returns data organized by subject and condition.\n","    \"\"\"\n","    if task == 'MI':\n","        return load_all_MI_data(root_dir, kwargs.get('label_dir'))\n","    elif task == 'SSVEP':\n","        return load_all_SSVEP_data(root_dir)\n","    elif task == 'P300':\n","        return load_all_P300_data(root_dir)\n","    elif task == 'Imagined_speech':\n","        return load_all_ImaginedSpeech_data(root_dir)\n","\n","\n","def load_all_MI_data(root_dir, label_dir=None):\n","    \"\"\"Load all MI subjects without splitting - reuse dataset.py loading logic\"\"\"\n","    import numpy as np\n","    import os\n","\n","    subject_data = {}\n","\n","    # Load all 9 subjects (A01-A09)\n","    for subject_id in range(1, 10):\n","        try:\n","            # Load both sessions for each subject\n","            X, Y = MI_load_data_by_session(\n","                root_dir, subject_id,\n","                [\"first_session\"],\n","                label_dir\n","            )\n","\n","            # Convert to numpy\n","            X = X.cpu().numpy()  # (n_trials, n_channels, n_times)\n","            Y = Y.cpu().numpy()\n","\n","            # Group by condition\n","            subject_data[subject_id] = {}\n","            for label in np.unique(Y):\n","                mask = Y == label\n","                subject_data[subject_id][int(label)] = X[mask]\n","\n","            print(f\"Loaded Subject {subject_id}: {len(Y)} trials, {len(np.unique(Y))} conditions\")\n","\n","        except Exception as e:\n","            print(f\"Could not load subject {subject_id}: {e}\")\n","\n","    return subject_data\n","\n","\n","def load_all_SSVEP_data(root_dir):\n","    \"\"\"Load all SSVEP subjects without splitting\"\"\"\n","    import numpy as np\n","    import os\n","\n","    subject_data = {}\n","\n","    # Load all 35 subjects\n","    for subject_id in range(1, 36):\n","        try:\n","            # Load all 6 sessions\n","            X, Y = load_data_by_session(root_dir, subject_id, [0, 1, 2, 3, 4, 5])\n","\n","            X = X.cpu().numpy()\n","            Y = Y.cpu().numpy()\n","\n","            # Group by condition (26 letters)\n","            subject_data[subject_id] = {}\n","            for label in np.unique(Y):\n","                mask = Y == label\n","                subject_data[subject_id][int(label)] = X[mask]\n","\n","            print(f\"Loaded Subject {subject_id}: {len(Y)} trials\")\n","\n","        except Exception as e:\n","            print(f\"Could not load subject {subject_id}: {e}\")\n","\n","    return subject_data\n","\n","\n","def load_all_P300_data(root_dir):\n","    \"\"\"Load all P300 subjects without splitting\"\"\"\n","    import numpy as np\n","\n","    subject_data = {}\n","\n","    # Load all subjects (typically 1-8)\n","    for subject_id in range(1, 44):\n","        try:\n","            data = P300_load_subject_data(subject_id, root_dir)\n","\n","            X = data['X']  # Already numpy\n","            Y = data['Y']\n","\n","            # Group by condition\n","            subject_data[subject_id] = {}\n","            for label in np.unique(Y):\n","                mask = Y == label\n","                subject_data[subject_id][int(label)] = X[mask]\n","\n","            print(f\"Loaded Subject {subject_id}: {len(Y)} trials\")\n","\n","        except Exception as e:\n","            print(f\"Could not load subject {subject_id}: {e}\")\n","\n","    return subject_data\n","\n","\n","def load_all_ImaginedSpeech_data(root_dir):\n","    \"\"\"Load all Imagined Speech subjects without splitting\"\"\"\n","    import numpy as np\n","    import os\n","    import re\n","\n","    subject_data = {}\n","\n","    # Find all available subjects from file names\n","    all_subjects = sorted([\n","        re.findall(r'epochs_(.*)\\.npy', f)[0].replace(\"_notched\", \"\")\n","        for f in os.listdir(root_dir)\n","        if f.startswith(\"epochs_\") and f.endswith(\".npy\")\n","    ])\n","\n","    for subject_id in all_subjects:\n","        try:\n","            data = ImaginedSpeech_load_subject_data(subject_id, root_dir)\n","\n","            X = data['X']  # Already numpy\n","            Y = data['Y']\n","\n","            # Group by condition\n","            subject_data[subject_id] = {}\n","            for label in np.unique(Y):\n","                mask = Y == label\n","                subject_data[subject_id][int(label)] = X[mask]\n","\n","            print(f\"Loaded Subject {subject_id}: {len(Y)} trials\")\n","\n","        except Exception as e:\n","            print(f\"Could not load subject {subject_id}: {e}\")\n","\n","    return subject_data\n","\n","# ============================================================================\n","# KEEP YOUR EXISTING HELPER FUNCTIONS (from helpers.py)\n","# ============================================================================\n","\n","def find_mi_window(trial_data, sfreq, baseline_end_idx):\n","    \"\"\"YOUR EXISTING FUNCTION - no changes needed\"\"\"\n","    power = trial_data ** 2\n","    baseline_power = np.mean(power[:baseline_end_idx])\n","    baseline_std = np.std(power[:baseline_end_idx])\n","    threshold = baseline_power + 2 * baseline_std\n","    active = power > threshold\n","\n","    min_duration = int(0.5 * sfreq)\n","    active_start = None\n","\n","    for i in range(baseline_end_idx, len(active) - min_duration):\n","        if np.sum(active[i:i+min_duration]) > 0.7 * min_duration:\n","            active_start = i\n","            break\n","\n","    if active_start is None:\n","        active_start = baseline_end_idx + int(0.5 * sfreq)\n","\n","    active_end = min(active_start + int(1.5 * sfreq), len(trial_data))\n","    return active_start, active_end\n","\n","\n","def align_trials_to_mi_onset(data, sfreq, baseline_end_idx):\n","    \"\"\"YOUR EXISTING FUNCTION - no changes needed\"\"\"\n","    n_trials, n_times = data.shape\n","    aligned_trials = []\n","    windows = []\n","\n","    for trial in data:\n","        start, end = find_mi_window(trial, sfreq, baseline_end_idx)\n","        windows.append((start, end))\n","\n","    window_lengths = [end - start for start, end in windows]\n","    target_length = int(np.median(window_lengths))\n","\n","    for trial, (start, end) in zip(data, windows):\n","        if end - start >= target_length:\n","            aligned_trials.append(trial[start:start+target_length])\n","        else:\n","            segment = trial[start:end]\n","            padded = np.pad(segment, (0, target_length - len(segment)), mode='edge')\n","            aligned_trials.append(padded)\n","\n","    return np.array(aligned_trials)\n","\n","# ============================================================================\n","# YOUR EXISTING ANALYSIS FUNCTIONS - Minimal changes\n","# ============================================================================\n","\n","def compute_channel_consistency_temporal(trials, sfreq, use_alignment):\n","    \"\"\"\n","    Modified version of your function to work with numpy arrays directly.\n","\n","    Args:\n","        trials: numpy array (n_trials, n_channels, n_times)\n","        sfreq: sampling frequency\n","        use_alignment: whether to align trials (only for MI)\n","    \"\"\"\n","    n_trials, n_channels, n_times = trials.shape\n","\n","    if n_trials < 2:\n","        return np.zeros(n_channels)\n","\n","    baseline_end_idx = int(1.0 * sfreq)  # Assuming 1s baseline for MI\n","    max_shift_samples = int(0.2 * sfreq)\n","\n","    channel_consistencies = []\n","\n","    for ch_idx in range(n_channels):\n","        channel_data = trials[:, ch_idx, :]  # (n_trials, n_times)\n","\n","        # Auto-align if requested (only for MI)\n","        if use_alignment:\n","            aligned_data = align_trials_to_mi_onset(channel_data, sfreq, baseline_end_idx)\n","        else:\n","            aligned_data = channel_data\n","\n","        # Compute pairwise correlations (YOUR EXISTING LOGIC)\n","        correlations = []\n","        for i in range(n_trials):\n","            for j in range(i+1, n_trials):\n","                trial1 = aligned_data[i]\n","                trial2 = aligned_data[j]\n","\n","                # Normalize\n","                trial1 = (trial1 - np.mean(trial1)) / (np.std(trial1) + 1e-10)\n","                trial2 = (trial2 - np.mean(trial2)) / (np.std(trial2) + 1e-10)\n","\n","                # Cross-correlation (YOUR EXISTING LOGIC)\n","                xcorr = np.correlate(trial1, trial2, mode='same')\n","                xcorr = xcorr / len(trial1)\n","\n","                center = len(xcorr) // 2\n","                start = max(0, center - max_shift_samples)\n","                end = min(len(xcorr), center + max_shift_samples)\n","                max_corr = np.max(xcorr[start:end])\n","\n","                correlations.append(max_corr)\n","\n","        channel_consistencies.append(np.mean(correlations))\n","\n","    return np.array(channel_consistencies)\n","\n","\n","def compute_channel_consistency_frequency(trials, sfreq, fmin, fmax):\n","    \"\"\"\n","    Modified to work with numpy arrays instead of MNE Epochs.\n","\n","    Args:\n","        trials: numpy array (n_trials, n_channels, n_times)\n","        sfreq: sampling frequency\n","        fmin, fmax: frequency band\n","    \"\"\"\n","    from scipy import signal as sp_signal\n","\n","    n_trials, n_channels, n_times = trials.shape\n","\n","    if n_trials < 2:\n","        return np.zeros(n_channels)\n","\n","    channel_consistencies = []\n","\n","    for ch_idx in range(n_channels):\n","        channel_data = trials[:, ch_idx, :]  # (n_trials, n_times)\n","\n","        # Compute PSD for each trial\n","        psds = []\n","        for trial in channel_data:\n","            freqs, psd = sp_signal.welch(trial, fs=sfreq, nperseg=min(512, n_times))\n","            # Select frequency band\n","            freq_mask = (freqs >= fmin) & (freqs <= fmax)\n","            psds.append(psd[freq_mask])\n","\n","        psds = np.array(psds)  # (n_trials, n_freqs)\n","\n","        # Compute pairwise correlations (YOUR EXISTING LOGIC)\n","        corr_matrix = np.corrcoef(psds)\n","        triu_idx = np.triu_indices_from(corr_matrix, k=1)\n","        pairwise_corrs = corr_matrix[triu_idx]\n","\n","        channel_consistencies.append(np.mean(pairwise_corrs))\n","\n","    return np.array(channel_consistencies)\n","\n","\n","def analyze_subject(subject_data, subject_id, task_type):\n","    \"\"\"\n","    Modified to work with numpy arrays from DataLoader.\n","\n","    Args:\n","        subject_data: {condition_id: numpy_array(n_trials, n_channels, n_times)}\n","        subject_id: subject identifier\n","        task_type: task name\n","    \"\"\"\n","    print(f\"\\nAnalyzing Subject {subject_id}...\")\n","\n","    config = TASK_CONFIG[task_type]\n","    sfreq = config['sfreq']\n","    use_alignment = config['use_alignment']\n","\n","    # Get channel names (generic for now)\n","    first_condition = list(subject_data.values())[0]\n","    n_channels = first_condition.shape[1]\n","    channel_names = [f'Ch{i}' for i in range(n_channels)]\n","\n","    results = {\n","        'subject': subject_id,\n","        'task': task_type,\n","        'channel_names': channel_names,\n","        'conditions': {}\n","    }\n","\n","    for condition_id, trials in subject_data.items():\n","        n_trials = trials.shape[0]\n","\n","        if n_trials < 2:\n","            print(f\"  Skipping condition {condition_id}: only {n_trials} trial(s)\")\n","            continue\n","\n","        print(f\"  Condition {condition_id}: {n_trials} trials\")\n","\n","        # Temporal consistency\n","        temp_consistency = compute_channel_consistency_temporal(trials, sfreq, use_alignment)\n","\n","        # Frequency consistency per band\n","        freq_consistency = {}\n","        for band_name, (fmin, fmax) in freq_bands.items():\n","            freq_consistency[band_name] = compute_channel_consistency_frequency(\n","                trials, sfreq, fmin, fmax\n","            )\n","\n","        results['conditions'][condition_id] = {\n","            'n_trials': n_trials,\n","            'temporal': temp_consistency,\n","            'frequency': freq_consistency\n","        }\n","\n","    return results\n","\n","# ============================================================================\n","# KEEP YOUR EXISTING PLOTTING FUNCTIONS - minimal changes\n","# ============================================================================\n","\n","def plot_subject_channel_consistency(results, save_path):\n","    \"\"\"YOUR EXISTING FUNCTION - just adjusted for condition_id instead of names\"\"\"\n","    subject_id = results['subject']\n","    channel_names = results['channel_names']\n","    conditions = list(results['conditions'].keys())\n","\n","    if not conditions:\n","        print(f\"No conditions to plot for subject {subject_id}\")\n","        return\n","\n","    n_conds = min(len(conditions), 4)\n","    n_bands = len(freq_bands)\n","    n_freq_rows = (n_bands + 1) // 2  # 2 bands per row\n","    total_rows = 2 + n_freq_rows      # 2 rows for temporal + freq rows\n","\n","    fig = plt.figure(figsize=(24, 6 * total_rows))\n","    gs = fig.add_gridspec(total_rows, 4, hspace=0.4, wspace=0.4)\n","\n","    # Plot temporal consistency per channel for each condition\n","    subplot_positions = [(0, 0), (0, 1), (1, 0), (1, 1)]\n","    for idx, condition_id in enumerate(conditions[:4]):\n","        row, col = subplot_positions[idx]\n","        ax = fig.add_subplot(gs[row, col])\n","\n","        temp_scores = results['conditions'][condition_id]['temporal']\n","\n","        x = np.arange(len(channel_names))\n","        colors = ['red' if score < 0.5 else 'orange' if score < 0.7 else 'green'\n","                  for score in temp_scores]\n","        bars = ax.bar(x, temp_scores, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n","\n","        ax.axhline(0.7, color='green', linestyle='--', alpha=0.5, linewidth=2, label='High (>0.7)')\n","        ax.axhline(0.5, color='orange', linestyle='--', alpha=0.5, linewidth=2, label='Moderate (>0.5)')\n","        ax.set_xlabel('Channel Index')\n","        ax.set_ylabel('Mean Trial Correlation', fontsize=10)\n","        ax.set_title(f'Condition {condition_id} - Temporal Consistency', fontweight='bold', fontsize=11)\n","        ax.set_ylim([0, 1])\n","        ax.legend(loc='upper right', fontsize=8)\n","        ax.grid(True, alpha=0.3, axis='y')\n","\n","    # Frequency band comparison - ONE SUBPLOT PER BAND (bottom 2 rows)\n","    if len(conditions) > 0:\n","        band_names = list(freq_bands.keys())\n","\n","        for band_idx, band_name in enumerate(band_names):\n","            ax = fig.add_subplot(gs[2 + band_idx // 2, (band_idx % 2) * 2:(band_idx % 2) * 2 + 2])\n","\n","            # Collect scores for this band across all conditions\n","            x = np.arange(len(conditions))\n","            band_scores = []\n","\n","            for condition_id in conditions:\n","                freq_scores = results['conditions'][condition_id]['frequency'][band_name]\n","                band_scores.append(np.mean(freq_scores))\n","\n","            # Color bars by consistency level\n","            colors = ['red' if s < 0.5 else 'orange' if s < 0.7 else 'green' for s in band_scores]\n","            bars = ax.bar(x, band_scores, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\n","\n","            ax.axhline(0.7, color='green', linestyle='--', alpha=0.5, linewidth=2)\n","            ax.axhline(0.5, color='orange', linestyle='--', alpha=0.5, linewidth=2)\n","            ax.set_xlabel('Condition', fontsize=10)\n","            ax.set_ylabel('Mean Correlation', fontsize=10)\n","            ax.set_title(f'{band_name} Band ({freq_bands[band_name][0]}-{freq_bands[band_name][1]} Hz)',\n","                        fontweight='bold', fontsize=11)\n","            ax.set_xticks(x)\n","            ax.set_xticklabels([f'C{c}' for c in conditions])\n","            ax.set_ylim([0, 1])\n","            ax.grid(True, alpha=0.3, axis='y')\n","\n","    plt.suptitle(f'Subject {subject_id} - {results[\"task\"]} Task',\n","                 fontsize=16, fontweight='bold')\n","    plt.tight_layout()\n","    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","    plt.close()\n","    print(f\"Saved plot: {save_path}\")\n","\n","\n","def create_summary_dataframe(all_results):\n","    \"\"\"YOUR EXISTING FUNCTION - minimal changes\"\"\"\n","    summary_data = []\n","\n","    for result in all_results:\n","        subject = result['subject']\n","        task = result['task']\n","\n","        for condition_id, cond_data in result['conditions'].items():\n","            temp_consistency = cond_data['temporal']\n","\n","            row = {\n","                'Subject': subject,\n","                'Task': task,\n","                'Condition': condition_id,\n","                'N_Trials': cond_data['n_trials'],\n","                'Temporal_Mean': np.mean(temp_consistency),\n","                'Temporal_Std': np.std(temp_consistency),\n","            }\n","\n","            # Add frequency bands\n","            for band_name in freq_bands.keys():\n","                freq_scores = cond_data['frequency'][band_name]\n","                row[f'{band_name}_Mean'] = np.mean(freq_scores)\n","                row[f'{band_name}_Std'] = np.std(freq_scores)\n","\n","            summary_data.append(row)\n","\n","    return pd.DataFrame(summary_data)\n","\n","\n","def plot_cross_subject_summary(df, save_path):\n","    \"\"\"YOUR EXISTING FUNCTION - keep as-is\"\"\"\n","    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n","\n","    conditions = df['Condition'].unique()\n","\n","    # Plot 1: Temporal consistency by subject\n","    ax = axes[0, 0]\n","    for condition in conditions:\n","        data = df[df['Condition'] == condition]\n","        ax.plot(data['Subject'], data['Temporal_Mean'],\n","                marker='o', label=f'Cond {condition}', linewidth=2, markersize=8)\n","    ax.axhline(0.7, color='green', linestyle='--', alpha=0.3)\n","    ax.axhline(0.5, color='orange', linestyle='--', alpha=0.3)\n","    ax.set_xlabel('Subject')\n","    ax.set_ylabel('Mean Consistency')\n","    ax.set_title('Temporal Consistency Across Subjects', fontweight='bold')\n","    ax.legend(fontsize=8)\n","    ax.grid(True, alpha=0.3)\n","    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n","\n","    # Plot 2: Distribution by condition\n","    ax = axes[0, 1]\n","    temp_data = [df[df['Condition'] == c]['Temporal_Mean'].values for c in conditions]\n","    bp = ax.boxplot(temp_data, labels=[f'C{c}' for c in conditions], patch_artist=True)\n","    for patch in bp['boxes']:\n","        patch.set_facecolor('lightblue')\n","    ax.axhline(0.7, color='green', linestyle='--', alpha=0.3)\n","    ax.axhline(0.5, color='orange', linestyle='--', alpha=0.3)\n","    ax.set_ylabel('Consistency')\n","    ax.set_title('Distribution by Condition', fontweight='bold')\n","    ax.grid(True, alpha=0.3, axis='y')\n","\n","    # Plot 3: Subject ranking\n","    ax = axes[1, 0]\n","    subject_avg = df.groupby('Subject')['Temporal_Mean'].mean().sort_values(ascending=False)\n","    colors = ['green' if v > 0.7 else 'orange' if v > 0.5 else 'red'\n","              for v in subject_avg.values]\n","    ax.bar(range(len(subject_avg)), subject_avg.values, color=colors, alpha=0.7)\n","    ax.axhline(0.7, color='green', linestyle='--', alpha=0.3)\n","    ax.axhline(0.5, color='orange', linestyle='--', alpha=0.3)\n","    ax.set_xlabel('Subject (ranked)')\n","    ax.set_ylabel('Mean Consistency')\n","    ax.set_title('Subject Ranking', fontweight='bold')\n","    ax.set_xticks(range(len(subject_avg)))\n","    ax.set_xticklabels([f'S{s}' for s in subject_avg.index], rotation=45, ha='right')\n","    ax.grid(True, alpha=0.3, axis='y')\n","\n","    # Plot 4: Frequency bands heatmap\n","    ax = axes[1, 1]\n","    band_cols = [col for col in df.columns if col.endswith('_Mean') and col != 'Temporal_Mean']\n","    band_data = df.groupby('Condition')[band_cols].mean()\n","    band_data.columns = [col.replace('_Mean', '') for col in band_data.columns]\n","\n","    im = ax.imshow(band_data.values, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')\n","    ax.set_xticks(range(len(band_data.columns)))\n","    ax.set_xticklabels(band_data.columns)\n","    ax.set_yticks(range(len(band_data.index)))\n","    ax.set_yticklabels([f'C{i}' for i in band_data.index])\n","    ax.set_title('Frequency Band Consistency', fontweight='bold')\n","    plt.colorbar(im, ax=ax, label='Mean Correlation')\n","\n","    for i in range(len(band_data.index)):\n","        for j in range(len(band_data.columns)):\n","            ax.text(j, i, f'{band_data.values[i, j]:.2f}',\n","                   ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n","\n","    plt.suptitle(f'Cross-Subject Summary - {TASK} Task', fontsize=16, fontweight='bold')\n","    plt.tight_layout()\n","    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","    plt.close()\n","    print(\"Saved cross-subject summary\")\n","\n","\n","def print_report(df):\n","    \"\"\"YOUR EXISTING FUNCTION - keep as-is\"\"\"\n","    print(\"\\n\" + \"=\"*80)\n","    print(f\"CROSS-TRIAL CONSISTENCY ANALYSIS - {TASK} TASK\")\n","    print(\"=\"*80)\n","    print(f\"\\nTotal subjects: {df['Subject'].nunique()}\")\n","    print(f\"Total trials: {df['N_Trials'].sum()}\")\n","    print(f\"Total conditions: {df['Condition'].nunique()}\")\n","\n","    print(\"\\n\" + \"-\"*80)\n","    print(\"OVERALL CONSISTENCY:\")\n","    print(f\"  Mean: {df['Temporal_Mean'].mean():.3f} ± {df['Temporal_Mean'].std():.3f}\")\n","    print(f\"  Range: [{df['Temporal_Mean'].min():.3f}, {df['Temporal_Mean'].max():.3f}]\")\n","\n","    print(\"\\n\" + \"-\"*80)\n","    print(\"PER-CONDITION BREAKDOWN:\")\n","    for condition in df['Condition'].unique():\n","        cond_data = df[df['Condition'] == condition]\n","        print(f\"\\n  Condition {condition}:\")\n","        print(f\"    Temporal: {cond_data['Temporal_Mean'].mean():.3f} ± {cond_data['Temporal_Mean'].std():.3f}\")\n","\n","        # Best frequency band\n","        band_cols = [col for col in df.columns if col.endswith('_Mean') and col != 'Temporal_Mean']\n","        band_means = {col.replace('_Mean', ''): cond_data[col].mean() for col in band_cols}\n","        best_band = max(band_means, key=band_means.get)\n","        print(f\"    Best freq band: {best_band} ({band_means[best_band]:.3f})\")\n","\n","# ============================================================================\n","# MAIN EXECUTION\n","# ============================================================================\n","\n","if __name__ == \"__main__\":\n","    print(f\"\\n{'='*80}\")\n","    print(f\"Cross-Trial Consistency Analysis - {TASK} Task\")\n","    print(f\"{'='*80}\\n\")\n","\n","    # Load ALL data for all subjects (not split - this is for analysis, not training)\n","    root_dir = DATA_PATHS[TASK]\n","    print(f\"Loading ALL {TASK} data from: {root_dir}\")\n","\n","    kwargs = {}\n","\n","    subject_data = load_all_subjects_for_task(TASK, root_dir, **kwargs)\n","\n","    print(f\"\\n{'='*40}\")\n","    print(f\"Found {len(subject_data)} subjects\")\n","    for sid, conds in subject_data.items():\n","        print(f\"  Subject {sid}: {len(conds)} conditions, \", end='')\n","        total_trials = sum(trials.shape[0] for trials in conds.values())\n","        print(f\"{total_trials} total trials\")\n","    print(f\"{'='*40}\\n\")\n","\n","    # Analyze each subject\n","    all_results = []\n","\n","    for subject_id, conditions in subject_data.items():\n","        try:\n","            result = analyze_subject(conditions, subject_id, TASK)\n","            all_results.append(result)\n","\n","            # Plot individual subject\n","            plot_path = OUTPUT_DIR / f'subject_{subject_id}_consistency.png'\n","            plot_subject_channel_consistency(result, plot_path)\n","\n","        except Exception as e:\n","            print(f\"Error with subject {subject_id}: {e}\")\n","            import traceback\n","            traceback.print_exc()\n","\n","    # Create summary\n","    if all_results:\n","        df = create_summary_dataframe(all_results)\n","        df.to_csv(OUTPUT_DIR / 'consistency_summary.csv', index=False)\n","        print(\"\\nSaved CSV summary\")\n","\n","        summary_plot_path = OUTPUT_DIR / 'cross_trial_summary.png'\n","        plot_cross_subject_summary(df, summary_plot_path)\n","\n","        # Print report\n","        print_report(df)\n","\n","        print(f\"\\n{'='*80}\")\n","        print(f\"Analysis complete! Results saved to: {OUTPUT_DIR}\")\n","        print(f\"{'='*80}\\n\")\n","    else:\n","        print(\"\\nNo results to summarize!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"executionInfo":{"elapsed":2357,"status":"error","timestamp":1761191879377,"user":{"displayName":"Megan Lee","userId":"00067592574554079062"},"user_tz":240},"id":"OnO0zZXzHfDr","outputId":"6f254ceb-4261-4c62-cd0b-3456f9cbc1fa"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","================================================================================\n","Channel Selection Analysis - P300 Task\n","================================================================================\n","\n"]},{"ename":"NameError","evalue":"name 'DATA_PATHS' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3135815191.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATA_PATHS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading {TASK} data from: {root_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DATA_PATHS' is not defined"]}],"source":["\"\"\"\n","Unified Channel Selection Analysis for All EEG Task Types\n","Identifies which channels are most distinct and should be kept for modeling.\n","\n","Only creates essential plots:\n","- Per subject: Incremental selection strategy\n","- Cross-subject: All channels ranked\n","\"\"\"\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from pathlib import Path\n","from scipy.spatial.distance import squareform\n","from scipy.cluster import hierarchy\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","TASK = 'P300'  # Change to: 'MI', 'SSVEP', 'P300', 'Imagined_speech'\n","\n","OUTPUT_DIR = Path(f'/content/drive/MyDrive/IDL/IDL Project Team 5 F25/data analysis/{TASK}/channel_selection_{TASK}')\n","OUTPUT_DIR.mkdir(exist_ok=True)\n","\n","TASK_CONFIG = {\n","    'MI': {'sfreq': 250, 'n_classes': 4},\n","    'SSVEP': {'sfreq': 250, 'n_classes': 26},\n","    'P300': {'sfreq': 256, 'n_classes': 2},\n","    'Imagined_speech': {'sfreq': 128, 'n_classes': 11}\n","}\n","\n","# Efficiency settings\n","MAX_TRIALS_FOR_ANALYSIS = 50  # Subsample trials if more\n","\n","# ============================================================================\n","# CHANNEL SELECTION ANALYSIS\n","# ============================================================================\n","\n","def compute_channel_similarity(trial_data):\n","    \"\"\"\n","    Compute correlation between all channel pairs within a single trial.\n","\n","    Args:\n","        trial_data: (n_channels, n_times) - single trial\n","\n","    Returns:\n","        similarity_matrix: (n_channels, n_channels)\n","    \"\"\"\n","    return np.corrcoef(trial_data)\n","\n","\n","def identify_representative_channels(trials, max_trials=50):\n","    \"\"\"\n","    Identify which channels are most representative and should be kept.\n","\n","    Args:\n","        trials: (n_trials, n_channels, n_times)\n","        max_trials: subsample if more\n","\n","    Returns:\n","        Dictionary with:\n","        - diversity_ranking: channels ranked by distinctiveness\n","        - incremental_selection: ordered list of channels to keep\n","        - avg_similarity_matrix: average channel-channel correlation\n","    \"\"\"\n","    n_trials, n_channels, n_times = trials.shape\n","\n","    # Subsample if needed\n","    if n_trials > max_trials:\n","        print(f\"    Subsampling {n_trials} -> {max_trials} trials for efficiency\")\n","        indices = np.random.choice(n_trials, max_trials, replace=False)\n","        trials = trials[indices]\n","        n_trials = max_trials\n","\n","    # Compute average similarity across all trials\n","    print(f\"    Computing channel similarities across {n_trials} trials...\")\n","    trial_similarities = []\n","    for trial_idx in range(n_trials):\n","        sim_matrix = compute_channel_similarity(trials[trial_idx])\n","        trial_similarities.append(sim_matrix)\n","\n","    avg_similarity = np.mean(trial_similarities, axis=0)\n","\n","    # Method 1: DIVERSITY SCORE - channels least correlated with others\n","    diversity_scores = []\n","    for i in range(n_channels):\n","        # Lower average correlation = more unique information\n","        avg_corr = np.mean(np.abs(avg_similarity[i, :]))\n","        diversity_score = 1 - avg_corr\n","        diversity_scores.append({\n","            'channel_idx': i,\n","            'diversity_score': diversity_score,\n","            'avg_correlation': avg_corr\n","        })\n","\n","    # Sort by diversity (highest = most distinct = keep first)\n","    diversity_scores = sorted(diversity_scores, key=lambda x: x['diversity_score'], reverse=True)\n","\n","    # Method 2: INCREMENTAL SELECTION - maximize diversity while adding channels\n","    selected_indices = []\n","\n","    # Start with most diverse channel\n","    first_idx = diversity_scores[0]['channel_idx']\n","    selected_indices.append(first_idx)\n","\n","    # Incrementally add channels that are least correlated with already selected\n","    for _ in range(min(n_channels - 1, 14)):  # Select up to 15 total\n","        best_score = -1\n","        best_idx = None\n","\n","        for d in diversity_scores:\n","            ch_idx = d['channel_idx']\n","            if ch_idx in selected_indices:\n","                continue\n","\n","            # Calculate average correlation with already selected channels\n","            avg_corr_with_selected = np.mean([np.abs(avg_similarity[ch_idx, si])\n","                                             for si in selected_indices])\n","\n","            score = 1 - avg_corr_with_selected\n","            if score > best_score:\n","                best_score = score\n","                best_idx = ch_idx\n","\n","        if best_idx is not None:\n","            selected_indices.append(best_idx)\n","\n","    return {\n","        'diversity_ranking': diversity_scores,\n","        'incremental_selection': selected_indices,\n","        'avg_similarity_matrix': avg_similarity\n","    }\n","\n","\n","def analyze_subject_channels(subject_data, subject_id, task_type):\n","    \"\"\"\n","    Analyze channel selection for one subject across all conditions.\n","\n","    Args:\n","        subject_data: {condition_id: numpy_array(n_trials, n_channels, n_times)}\n","        subject_id: subject identifier\n","        task_type: task name\n","\n","    Returns:\n","        Dictionary with channel rankings and selections\n","    \"\"\"\n","    print(f\"\\nAnalyzing Subject {subject_id} (channel selection)...\")\n","\n","    # Collect all trials across conditions for this subject\n","    all_trials = []\n","    for condition_id, trials in subject_data.items():\n","        all_trials.append(trials)\n","        print(f\"  Condition {condition_id}: {trials.shape[0]} trials\")\n","\n","    # Concatenate all trials\n","    combined_trials = np.concatenate(all_trials, axis=0)\n","    n_trials, n_channels, n_times = combined_trials.shape\n","\n","    print(f\"  Total: {n_trials} trials, {n_channels} channels\")\n","\n","    # Analyze channel selection\n","    results = identify_representative_channels(\n","        combined_trials,\n","        max_trials=MAX_TRIALS_FOR_ANALYSIS\n","    )\n","\n","    results['subject'] = subject_id\n","    results['task'] = task_type\n","    results['n_channels'] = n_channels\n","    results['n_trials'] = n_trials\n","\n","    return results\n","\n","# ============================================================================\n","# SIMPLIFIED PLOTTING - Only essential plots\n","# ============================================================================\n","\n","def plot_subject_incremental_selection(results, save_path):\n","    \"\"\"\n","    Plot ONLY the incremental selection strategy for one subject.\n","    This is the row 2, left plot from your original visualization.\n","    \"\"\"\n","    subject_id = results['subject']\n","    incremental_indices = results['incremental_selection'][:15]\n","    n_channels = results['n_channels']\n","\n","    # Create channel names\n","    channel_names = [f'Ch{i}' for i in range(n_channels)]\n","    inc_channels = [channel_names[idx] for idx in incremental_indices]\n","\n","    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n","\n","    y_pos = np.arange(len(inc_channels))\n","    ax.barh(y_pos, range(len(inc_channels), 0, -1),\n","            color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\n","    ax.set_yticks(y_pos)\n","    ax.set_yticklabels(inc_channels, fontsize=11)\n","    ax.set_xlabel('Selection Priority (higher = more important)', fontsize=13, fontweight='bold')\n","    ax.set_title(f'Subject {subject_id} - Incremental Channel Selection\\n'\n","                 f'(Maximizes diversity at each step)',\n","                fontsize=14, fontweight='bold')\n","    ax.invert_yaxis()\n","    ax.grid(True, alpha=0.3, axis='x')\n","\n","    # Add selection cutoff lines\n","    ax.axhline(4.5, color='red', linestyle='--', linewidth=2, label='Top 5')\n","    ax.axhline(9.5, color='orange', linestyle='--', linewidth=2, label='Top 10')\n","    ax.legend(fontsize=11)\n","\n","    # Add text annotation\n","    top5 = ', '.join([f'Ch{incremental_indices[i]}' for i in range(min(5, len(incremental_indices)))])\n","    ax.text(0.02, 0.98, f'Top 5 Channels: {top5}',\n","            transform=ax.transAxes, fontsize=10,\n","            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n","\n","    plt.tight_layout()\n","    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","    plt.close()\n","    print(f\"Saved: {save_path}\")\n","\n","\n","def create_cross_subject_summary(all_results):\n","    \"\"\"\n","    Create summary of channel rankings across all subjects.\n","    \"\"\"\n","    n_channels = all_results[0]['n_channels']\n","    channel_scores = {i: [] for i in range(n_channels)}\n","\n","    # Collect scores for each channel across subjects\n","    for result in all_results:\n","        for ch_data in result['diversity_ranking']:\n","            ch_idx = ch_data['channel_idx']\n","            channel_scores[ch_idx].append(ch_data['diversity_score'])\n","\n","    # Calculate statistics\n","    channel_summary = []\n","    for ch_idx in range(n_channels):\n","        scores = channel_scores[ch_idx]\n","        channel_summary.append({\n","            'channel_idx': ch_idx,\n","            'channel': f'Ch{ch_idx}',\n","            'avg_score': np.mean(scores),\n","            'std_score': np.std(scores),\n","            'min_score': np.min(scores),\n","            'max_score': np.max(scores)\n","        })\n","\n","    # Sort by average score\n","    channel_summary = sorted(channel_summary, key=lambda x: x['avg_score'], reverse=True)\n","\n","    return channel_summary\n","\n","\n","def plot_cross_subject_rankings(channel_summary, save_path):\n","    \"\"\"\n","    Plot ONLY the all channels ranked visualization.\n","    This is the bottom left plot from your original visualization.\n","    \"\"\"\n","    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","\n","    all_channels = [c['channel'] for c in channel_summary]\n","    all_scores = [c['avg_score'] for c in channel_summary]\n","\n","    # Color code channels\n","    colors = ['darkgreen' if i < 5 else 'green' if i < 10 else 'yellowgreen' if i < 15\n","              else 'orange' if i < 20 else 'red' for i in range(len(all_channels))]\n","\n","    ax.bar(range(len(all_channels)), all_scores, color=colors, alpha=0.7, edgecolor='black')\n","\n","    # Add reference lines\n","    ax.axhline(y=np.mean(all_scores), color='black', linestyle='--', linewidth=2,\n","              label=f'Average: {np.mean(all_scores):.3f}')\n","    ax.axvline(x=4.5, color='red', linestyle='--', alpha=0.5, linewidth=2)\n","    ax.axvline(x=9.5, color='orange', linestyle='--', alpha=0.5, linewidth=2)\n","    ax.axvline(x=14.5, color='yellow', linestyle='--', alpha=0.5, linewidth=2)\n","\n","    ax.set_xlabel('Channel (ranked)', fontsize=13, fontweight='bold')\n","    ax.set_ylabel('Average Distinctiveness', fontsize=13, fontweight='bold')\n","    ax.set_title(f'All Channels Ranked by Distinctiveness - {TASK} Task',\n","                 fontsize=15, fontweight='bold')\n","    ax.set_xticks(range(len(all_channels)))\n","    ax.set_xticklabels(all_channels, rotation=90, fontsize=9)\n","    ax.legend(fontsize=12)\n","    ax.grid(True, alpha=0.3, axis='y')\n","\n","    # Add text box with recommendations\n","    top5 = ', '.join([c['channel'] for c in channel_summary[:5]])\n","    top10 = ', '.join([c['channel'] for c in channel_summary[:10]])\n","    remove = ', '.join([c['channel'] for c in channel_summary[-5:]])\n","\n","    textstr = f'Top 5: {top5}\\nTop 10: {top10}\\nRemove: {remove}'\n","    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n","    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n","            verticalalignment='top', bbox=props, family='monospace')\n","\n","    plt.tight_layout()\n","    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","    plt.close()\n","    print(f\"Saved: {save_path}\")\n","\n","# ============================================================================\n","# MAIN EXECUTION\n","# ============================================================================\n","\n","if __name__ == \"__main__\":\n","    print(f\"\\n{'='*80}\")\n","    print(f\"Channel Selection Analysis - {TASK} Task\")\n","    print(f\"{'='*80}\\n\")\n","\n","    # Load data\n","    root_dir = DATA_PATHS[TASK]\n","    print(f\"Loading {TASK} data from: {root_dir}\")\n","\n","    kwargs = {}\n","    if TASK == 'MI':\n","        kwargs['label_dir'] = MI_LABEL_DIR\n","\n","    subject_data = load_all_subjects_for_task(TASK, root_dir, **kwargs)\n","\n","    print(f\"\\n{'='*40}\")\n","    print(f\"Found {len(subject_data)} subjects\")\n","    print(f\"{'='*40}\\n\")\n","\n","    # Analyze each subject\n","    all_results = []\n","\n","    for subject_id, conditions in subject_data.items():\n","        try:\n","            result = analyze_subject_channels(conditions, subject_id, TASK)\n","            all_results.append(result)\n","\n","            # Plot individual subject (simplified)\n","            plot_path = OUTPUT_DIR / f'subject_{subject_id}_channel_selection.png'\n","            plot_subject_incremental_selection(result, plot_path)\n","\n","        except Exception as e:\n","            print(f\"Error with subject {subject_id}: {e}\")\n","            import traceback\n","            traceback.print_exc()\n","\n","    # Create cross-subject summary\n","    if all_results:\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"GENERATING CROSS-SUBJECT SUMMARY\")\n","        print(\"=\"*80)\n","\n","        channel_summary = create_cross_subject_summary(all_results)\n","\n","        # Save CSV\n","        df = pd.DataFrame(channel_summary)\n","        df.to_csv(OUTPUT_DIR / 'channel_rankings.csv', index=False)\n","        print(f\"\\nSaved: {OUTPUT_DIR / 'channel_rankings.csv'}\")\n","\n","        # Plot summary (simplified)\n","        plot_path = OUTPUT_DIR / 'cross_subject_channel_rankings.png'\n","        plot_cross_subject_rankings(channel_summary, plot_path)\n","\n","        # Save text recommendations\n","        summary_path = OUTPUT_DIR / 'channel_recommendations.txt'\n","        with open(summary_path, 'w') as f:\n","            f.write(\"=\"*80 + \"\\n\")\n","            f.write(f\"CHANNEL RECOMMENDATIONS - {TASK} TASK\\n\")\n","            f.write(\"=\"*80 + \"\\n\\n\")\n","\n","            f.write(\"RANKED CHANNELS:\\n\")\n","            f.write(\"-\"*80 + \"\\n\")\n","            for i, ch_data in enumerate(channel_summary, 1):\n","                f.write(f\"{i:3d}. {ch_data['channel']:10s} | \"\n","                       f\"Score: {ch_data['avg_score']:.4f} ± {ch_data['std_score']:.4f}\\n\")\n","\n","            f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n","            f.write(\"RECOMMENDATIONS:\\n\")\n","            f.write(\"=\"*80 + \"\\n\")\n","            f.write(f\"\\nTop 5 (Must Keep):  {', '.join([c['channel'] for c in channel_summary[:5]])}\\n\")\n","            f.write(f\"\\nTop 10 (Recommended): {', '.join([c['channel'] for c in channel_summary[:10]])}\\n\")\n","            f.write(f\"\\nTop 15 (Conservative): {', '.join([c['channel'] for c in channel_summary[:15]])}\\n\")\n","            f.write(f\"\\nRemove (Lowest Priority): {', '.join([c['channel'] for c in channel_summary[-5:]])}\\n\")\n","\n","        print(f\"Saved: {summary_path}\")\n","\n","        print(f\"\\n{'='*80}\")\n","        print(f\"Analysis complete! Results saved to: {OUTPUT_DIR}\")\n","        print(f\"{'='*80}\\n\")\n","\n","        # Print quick summary\n","        print(\"\\nQUICK SUMMARY:\")\n","        print(f\"  Top 5:  {', '.join([c['channel'] for c in channel_summary[:5]])}\")\n","        print(f\"  Top 10: {', '.join([c['channel'] for c in channel_summary[:10]])}\")\n","\n","    else:\n","        print(\"\\nNo results to summarize!\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ix6Tv5vRMxgX","outputId":"59a90fd0-457c-43bf-8249-2f33c67f2fa2","executionInfo":{"status":"error","timestamp":1761272604374,"user_tz":240,"elapsed":4943569,"user":{"displayName":"Megan Lee","userId":"00067592574554079062"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","Cross-Subject Consistency Analysis - P300 Task\n","================================================================================\n","\n","Loading ALL P300 data from: /content/drive/MyDrive/IDL/IDL Project Team 5 F25/dataset/p300/bi2015a/cleaned_data\n","Loaded Subject 1: 4956 trials\n","Loaded Subject 2: 1512 trials\n","Loaded Subject 3: 1044 trials\n","Loaded Subject 4: 1440 trials\n","Loaded Subject 5: 1188 trials\n","Loaded Subject 6: 1584 trials\n","Loaded Subject 7: 1224 trials\n","Loaded Subject 8: 1512 trials\n","Loaded Subject 9: 1332 trials\n","Loaded Subject 10: 1116 trials\n","Loaded Subject 11: 1440 trials\n","Loaded Subject 12: 2232 trials\n","Loaded Subject 13: 1368 trials\n","Loaded Subject 14: 1404 trials\n","Loaded Subject 15: 1260 trials\n","Loaded Subject 16: 1260 trials\n","Loaded Subject 17: 1620 trials\n","Loaded Subject 18: 1620 trials\n","Loaded Subject 19: 1800 trials\n","Loaded Subject 20: 1620 trials\n","Loaded Subject 21: 1260 trials\n","Loaded Subject 22: 1584 trials\n","Loaded Subject 23: 1368 trials\n","Loaded Subject 24: 1296 trials\n","Loaded Subject 25: 2088 trials\n","Loaded Subject 26: 1116 trials\n","Loaded Subject 27: 2844 trials\n","Loaded Subject 28: 1800 trials\n","Loaded Subject 29: 1836 trials\n","Loaded Subject 30: 972 trials\n","Loaded Subject 31: 2700 trials\n","Loaded Subject 32: 1116 trials\n","Loaded Subject 33: 1512 trials\n","Loaded Subject 34: 1260 trials\n","Loaded Subject 35: 2484 trials\n","Loaded Subject 36: 1440 trials\n","Loaded Subject 37: 2520 trials\n","Loaded Subject 38: 1332 trials\n","Loaded Subject 39: 1764 trials\n","Loaded Subject 40: 1944 trials\n","Loaded Subject 41: 1620 trials\n","Loaded Subject 42: 2052 trials\n","Loaded Subject 43: 1044 trials\n","\n","========================================\n","Found 43 subjects\n","  Subject 1: 2 conditions, 4956 total trials\n","  Subject 2: 2 conditions, 1512 total trials\n","  Subject 3: 2 conditions, 1044 total trials\n","  Subject 4: 2 conditions, 1440 total trials\n","  Subject 5: 2 conditions, 1188 total trials\n","  Subject 6: 2 conditions, 1584 total trials\n","  Subject 7: 2 conditions, 1224 total trials\n","  Subject 8: 2 conditions, 1512 total trials\n","  Subject 9: 2 conditions, 1332 total trials\n","  Subject 10: 2 conditions, 1116 total trials\n","  Subject 11: 2 conditions, 1440 total trials\n","  Subject 12: 2 conditions, 2232 total trials\n","  Subject 13: 2 conditions, 1368 total trials\n","  Subject 14: 2 conditions, 1404 total trials\n","  Subject 15: 2 conditions, 1260 total trials\n","  Subject 16: 2 conditions, 1260 total trials\n","  Subject 17: 2 conditions, 1620 total trials\n","  Subject 18: 2 conditions, 1620 total trials\n","  Subject 19: 2 conditions, 1800 total trials\n","  Subject 20: 2 conditions, 1620 total trials\n","  Subject 21: 2 conditions, 1260 total trials\n","  Subject 22: 2 conditions, 1584 total trials\n","  Subject 23: 2 conditions, 1368 total trials\n","  Subject 24: 2 conditions, 1296 total trials\n","  Subject 25: 2 conditions, 2088 total trials\n","  Subject 26: 2 conditions, 1116 total trials\n","  Subject 27: 2 conditions, 2844 total trials\n","  Subject 28: 2 conditions, 1800 total trials\n","  Subject 29: 2 conditions, 1836 total trials\n","  Subject 30: 2 conditions, 972 total trials\n","  Subject 31: 2 conditions, 2700 total trials\n","  Subject 32: 2 conditions, 1116 total trials\n","  Subject 33: 2 conditions, 1512 total trials\n","  Subject 34: 2 conditions, 1260 total trials\n","  Subject 35: 2 conditions, 2484 total trials\n","  Subject 36: 2 conditions, 1440 total trials\n","  Subject 37: 2 conditions, 2520 total trials\n","  Subject 38: 2 conditions, 1332 total trials\n","  Subject 39: 2 conditions, 1764 total trials\n","  Subject 40: 2 conditions, 1944 total trials\n","  Subject 41: 2 conditions, 1620 total trials\n","  Subject 42: 2 conditions, 2052 total trials\n","  Subject 43: 2 conditions, 1044 total trials\n","========================================\n","\n","\n","================================================================================\n","STARTING Q4 ANALYSIS\n","================================================================================\n","\n","================================================================================\n","ANALYZING LABEL CONSISTENCY ACROSS SUBJECTS\n","================================================================================\n","\n","Precomputing subject patterns...\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","next\n","\n","Analyzing condition 0...\n","\n","Analyzing condition 1...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-765505379.py:543: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  bp = ax.boxplot(all_corrs, labels=[f'C{c}' for c in conditions], patch_artist=True)\n"]},{"output_type":"stream","name":"stdout","text":["Saved Q4 plot: /content/drive/MyDrive/IDL/IDL Project Team 5 F25/data analysis/P300/cross_subject_P300/q4_label_consistency.png\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'q3_results' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-765505379.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;31m# Generate report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m     \u001b[0mcreate_summary_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq3_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq4_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTASK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;31m# Save results to CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'q3_results' is not defined"]}],"source":["\"\"\"\n","Cross-Subject Analysis for All Tasks\n","Analyzes: MI, SSVEP, P300, Imagined Speech\n","\n","Questions:\n","Q3: Does the same channel for the same label look similar across subjects?\n","Q4: Are label representations consistent across subjects?\n","\"\"\"\n","\n","from itertools import combinations\n","from pathlib import Path\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# ============================================================================\n","# CONFIGURATION - Set your task here\n","# ============================================================================\n","\n","TASK = 'P300'  # Change to: 'MI', 'SSVEP', 'P300', 'Imagined_speech'\n","\n","OUTPUT_DIR = Path(f'/content/drive/MyDrive/IDL/IDL Project Team 5 F25/data analysis/{TASK}/cross_subject_{TASK}')\n","OUTPUT_DIR.mkdir(exist_ok=True)\n","\n","# ============================================================================\n","# ALIGNMENT AND PREPROCESSING FUNCTIONS\n","# ============================================================================\n","\n","def find_mi_window(trial_data, sfreq, baseline_end_idx):\n","    \"\"\"Find MI onset window (only used for MI task)\"\"\"\n","    power = trial_data ** 2\n","    baseline_power = np.mean(power[:baseline_end_idx])\n","    baseline_std = np.std(power[:baseline_end_idx])\n","    threshold = baseline_power + 2 * baseline_std\n","    active = power > threshold\n","\n","    min_duration = int(0.5 * sfreq)\n","    active_start = None\n","\n","    for i in range(baseline_end_idx, len(active) - min_duration):\n","        if np.sum(active[i:i+min_duration]) > 0.7 * min_duration:\n","            active_start = i\n","            break\n","\n","    if active_start is None:\n","        active_start = baseline_end_idx + int(0.5 * sfreq)\n","\n","    active_end = min(active_start + int(1.5 * sfreq), len(trial_data))\n","    return active_start, active_end\n","\n","\n","def align_trials_to_mi_onset(data, sfreq, baseline_end_idx):\n","    \"\"\"Align MI trials to onset (only used for MI task)\"\"\"\n","    n_trials = data.shape[0]\n","    aligned_trials = []\n","\n","    for trial_idx in range(n_trials):\n","        trial = data[trial_idx]\n","        start, end = find_mi_window(trial, sfreq, baseline_end_idx)\n","        aligned_trial = trial[start:end]\n","        aligned_trials.append(aligned_trial)\n","\n","    min_len = min(len(t) for t in aligned_trials)\n","    aligned_trials = [t[:min_len] for t in aligned_trials]\n","\n","    return np.array(aligned_trials)\n","\n","\n","def compute_subject_channel_average_temporal(data, condition, task, sfreq):\n","    \"\"\"\n","    Compute average temporal pattern per channel for a subject-condition pair.\n","\n","    Args:\n","        data: dict of condition -> numpy array (n_trials, n_channels, n_times)\n","        condition: condition label\n","        task: task name\n","        sfreq: sampling frequency\n","\n","    Returns: array of shape (n_channels, n_times_aligned)\n","    \"\"\"\n","    trial_data = data[condition]  # (n_trials, n_channels, n_times)\n","    n_channels = trial_data.shape[1]\n","\n","    use_alignment = TASK_CONFIG[task]['use_alignment']\n","\n","    channel_averages = []\n","\n","    for ch_idx in range(n_channels):\n","        channel_data = trial_data[:, ch_idx, :]  # (n_trials, n_times)\n","\n","        # Apply alignment only for MI task\n","        if use_alignment and task == 'MI':\n","            baseline_end_idx = int(1.0 * sfreq)\n","            aligned_data = align_trials_to_mi_onset(channel_data, sfreq, baseline_end_idx)\n","        else:\n","            aligned_data = channel_data\n","\n","        # Average across trials\n","        avg_pattern = np.mean(aligned_data, axis=0)\n","\n","        # Normalize\n","        avg_pattern = (avg_pattern - np.mean(avg_pattern)) / (np.std(avg_pattern) + 1e-10)\n","        channel_averages.append(avg_pattern)\n","\n","    # Find the minimum length across all channels\n","    min_length = min(len(pattern) for pattern in channel_averages)\n","\n","    # Truncate all patterns to the minimum length\n","    channel_averages_truncated = [pattern[:min_length] for pattern in channel_averages]\n","\n","    return np.array(channel_averages_truncated)\n","\n","\n","def compute_subject_channel_average_frequency(data, condition, fmin, fmax, sfreq):\n","    \"\"\"\n","    Compute average frequency pattern per channel for a subject-condition pair.\n","\n","    Args:\n","        data: dict of condition -> numpy array (n_trials, n_channels, n_times)\n","        condition: condition label\n","        fmin, fmax: frequency band\n","        sfreq: sampling frequency\n","\n","    Returns: array of shape (n_channels, n_freqs)\n","    \"\"\"\n","    from scipy import signal\n","\n","    trial_data = data[condition]  # (n_trials, n_channels, n_times)\n","    n_trials, n_channels, n_times = trial_data.shape\n","\n","    # Compute PSD using Welch's method\n","    psds_all = []\n","\n","    for ch_idx in range(n_channels):\n","        channel_data = trial_data[:, ch_idx, :]  # (n_trials, n_times)\n","\n","        trial_psds = []\n","        for trial_idx in range(n_trials):\n","            freqs, psd = signal.welch(\n","                channel_data[trial_idx],\n","                fs=sfreq,\n","                nperseg=min(512, n_times),\n","                noverlap=None\n","            )\n","\n","            # Filter to frequency band\n","            freq_mask = (freqs >= fmin) & (freqs <= fmax)\n","            trial_psds.append(psd[freq_mask])\n","\n","        # Average across trials\n","        avg_psd = np.mean(trial_psds, axis=0)\n","        psds_all.append(avg_psd)\n","\n","    psds_all = np.array(psds_all)  # (n_channels, n_freqs)\n","\n","    # Normalize each channel\n","    for ch_idx in range(psds_all.shape[0]):\n","        psds_all[ch_idx] = (psds_all[ch_idx] - np.mean(psds_all[ch_idx])) / (np.std(psds_all[ch_idx]) + 1e-10)\n","\n","    return psds_all\n","\n","# ============================================================================\n","# ANALYSIS FUNCTIONS\n","# ============================================================================\n","\n","def analyze_q3_channel_consistency(all_subject_data, task, sfreq):\n","    \"\"\"\n","    Q3: Does the same channel for the same label look similar across subjects?\n","\n","    For each condition and each channel, compare patterns across all subject pairs.\n","    \"\"\"\n","    results = {\n","        'temporal': {},\n","        'frequency': {band: {} for band in freq_bands.keys()}\n","    }\n","\n","    subject_list = list(all_subject_data.keys())\n","\n","    # Get all conditions (use first subject as reference)\n","    first_subject = subject_list[0]\n","    conditions = list(all_subject_data[first_subject].keys())\n","\n","    # Get number of channels (assume consistent across subjects)\n","    first_condition = conditions[0]\n","    n_channels = all_subject_data[first_subject][first_condition].shape[1]\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"ANALYZING CHANNEL CONSISTENCY ACROSS SUBJECTS\")\n","    print(\"=\"*80)\n","\n","    # Precompute all subject patterns for each condition\n","    print(\"\\nPrecomputing subject patterns...\")\n","    subject_patterns = {}\n","\n","    for subject_id in subject_list:\n","        print(f\"  Processing subject {subject_id}...\")\n","        subject_patterns[subject_id] = {}\n","\n","        for condition in conditions:\n","            print(f\"Condition {condition}\")\n","            if condition not in all_subject_data[subject_id]:\n","                continue\n","\n","            # Temporal patterns\n","            temporal = compute_subject_channel_average_temporal(\n","                all_subject_data[subject_id], condition, task, sfreq\n","            )\n","\n","            # Frequency patterns\n","            freq_patterns = {}\n","            for band_name, (fmin, fmax) in freq_bands.items():\n","                freq_patterns[band_name] = compute_subject_channel_average_frequency(\n","                    all_subject_data[subject_id], condition, fmin, fmax, sfreq\n","                )\n","\n","            subject_patterns[subject_id][condition] = {\n","                'temporal': temporal,\n","                'frequency': freq_patterns\n","            }\n","\n","    # Now compute correlations\n","    for condition in conditions:\n","        print(f\"\\nAnalyzing condition {condition}...\")\n","\n","        # Filter subjects that have this condition\n","        available_subjects = [s for s in subject_list if condition in subject_patterns[s]]\n","\n","        if len(available_subjects) < 2:\n","            print(f\"  Skipping condition {condition} - not enough subjects\")\n","            continue\n","\n","        # Temporal domain\n","        temporal_correlations = np.zeros((n_channels, len(available_subjects), len(available_subjects)))\n","\n","        for i, subj1 in enumerate(available_subjects):\n","            for j, subj2 in enumerate(available_subjects):\n","                pattern1 = subject_patterns[subj1][condition]['temporal']\n","                pattern2 = subject_patterns[subj2][condition]['temporal']\n","\n","                # Compare each channel\n","                for ch_idx in range(n_channels):\n","                    # Handle different lengths by truncating to shorter\n","                    len1, len2 = len(pattern1[ch_idx]), len(pattern2[ch_idx])\n","                    min_len = min(len1, len2)\n","\n","                    if min_len > 10:  # Minimum reasonable length\n","                        corr = np.corrcoef(\n","                            pattern1[ch_idx][:min_len],\n","                            pattern2[ch_idx][:min_len]\n","                        )[0, 1]\n","                        temporal_correlations[ch_idx, i, j] = corr\n","                    else:\n","                        temporal_correlations[ch_idx, i, j] = 0\n","\n","        # Extract upper triangle (unique pairs) for each channel\n","        channel_consistency_scores = []\n","        for ch_idx in range(n_channels):\n","            corr_matrix = temporal_correlations[ch_idx]\n","            triu_idx = np.triu_indices_from(corr_matrix, k=1)\n","            if len(triu_idx[0]) > 0:\n","                channel_consistency_scores.append(np.mean(corr_matrix[triu_idx]))\n","            else:\n","                channel_consistency_scores.append(0)\n","\n","        results['temporal'][condition] = {\n","            'channel_scores': np.array(channel_consistency_scores),\n","            'channel_names': [f'Ch{i+1}' for i in range(n_channels)],\n","            'mean': np.mean(channel_consistency_scores),\n","            'std': np.std(channel_consistency_scores)\n","        }\n","\n","        # Frequency domain\n","        for band_name, (fmin, fmax) in freq_bands.items():\n","            freq_correlations = np.zeros((n_channels, len(available_subjects), len(available_subjects)))\n","\n","            for i, subj1 in enumerate(available_subjects):\n","                for j, subj2 in enumerate(available_subjects):\n","                    pattern1 = subject_patterns[subj1][condition]['frequency'][band_name]\n","                    pattern2 = subject_patterns[subj2][condition]['frequency'][band_name]\n","\n","                    for ch_idx in range(n_channels):\n","                        corr = np.corrcoef(pattern1[ch_idx], pattern2[ch_idx])[0, 1]\n","                        freq_correlations[ch_idx, i, j] = corr\n","\n","            channel_consistency_scores = []\n","            for ch_idx in range(n_channels):\n","                corr_matrix = freq_correlations[ch_idx]\n","                triu_idx = np.triu_indices_from(corr_matrix, k=1)\n","                if len(triu_idx[0]) > 0:\n","                    channel_consistency_scores.append(np.mean(corr_matrix[triu_idx]))\n","                else:\n","                    channel_consistency_scores.append(0)\n","\n","            results['frequency'][band_name][condition] = {\n","                'channel_scores': np.array(channel_consistency_scores),\n","                'mean': np.mean(channel_consistency_scores),\n","                'std': np.std(channel_consistency_scores)\n","            }\n","\n","    return results\n","\n","\n","def analyze_q4_label_consistency(all_subject_data, task, sfreq):\n","    \"\"\"\n","    Q4: Are label representations consistent across subjects?\n","\n","    For each subject pair, compare how similar the same label is.\n","    \"\"\"\n","    results = {\n","        'temporal': {},\n","        'frequency': {band: {} for band in freq_bands.keys()}\n","    }\n","\n","    subject_list = list(all_subject_data.keys())\n","\n","    # Get all conditions\n","    first_subject = subject_list[0]\n","    conditions = list(all_subject_data[first_subject].keys())\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"ANALYZING LABEL CONSISTENCY ACROSS SUBJECTS\")\n","    print(\"=\"*80)\n","\n","    # Precompute all subject patterns\n","    print(\"\\nPrecomputing subject patterns...\")\n","    subject_patterns = {}\n","\n","    for subject_id in subject_list:\n","        subject_patterns[subject_id] = {}\n","\n","        for condition in conditions:\n","            if condition not in all_subject_data[subject_id]:\n","                continue\n","\n","            # Temporal patterns\n","            temporal = compute_subject_channel_average_temporal(\n","                all_subject_data[subject_id], condition, task, sfreq\n","            )\n","\n","            # Frequency patterns\n","            freq_patterns = {}\n","            for band_name, (fmin, fmax) in freq_bands.items():\n","                freq_patterns[band_name] = compute_subject_channel_average_frequency(\n","                    all_subject_data[subject_id], condition, fmin, fmax, sfreq\n","                )\n","\n","            subject_patterns[subject_id][condition] = {\n","                'temporal': temporal,\n","                'frequency': freq_patterns\n","            }\n","            print('next')\n","\n","    # Compute label consistency\n","    for condition in conditions:\n","        print(f\"\\nAnalyzing condition {condition}...\")\n","\n","        # Filter subjects that have this condition\n","        available_subjects = [s for s in subject_list if condition in subject_patterns[s]]\n","\n","        if len(available_subjects) < 2:\n","            print(f\"  Skipping condition {condition} - not enough subjects\")\n","            continue\n","\n","        # Temporal domain: average across all channels for whole-brain representation\n","        pairwise_correlations = []\n","\n","        for subj1, subj2 in combinations(available_subjects, 2):\n","            pattern1 = subject_patterns[subj1][condition]['temporal']\n","            pattern2 = subject_patterns[subj2][condition]['temporal']\n","\n","            # Flatten all channels and compute correlation\n","            pattern1_flat = pattern1.flatten()\n","            pattern2_flat = pattern2.flatten()\n","\n","            # Handle different lengths\n","            min_len = min(len(pattern1_flat), len(pattern2_flat))\n","\n","            if min_len > 100:  # Reasonable minimum\n","                corr = np.corrcoef(\n","                    pattern1_flat[:min_len],\n","                    pattern2_flat[:min_len]\n","                )[0, 1]\n","                pairwise_correlations.append(corr)\n","\n","        if pairwise_correlations:\n","            results['temporal'][condition] = {\n","                'correlations': np.array(pairwise_correlations),\n","                'mean': np.mean(pairwise_correlations),\n","                'std': np.std(pairwise_correlations),\n","                'median': np.median(pairwise_correlations)\n","            }\n","\n","        # Frequency domain\n","        for band_name, (fmin, fmax) in freq_bands.items():\n","            pairwise_correlations = []\n","\n","            for subj1, subj2 in combinations(available_subjects, 2):\n","                pattern1 = subject_patterns[subj1][condition]['frequency'][band_name]\n","                pattern2 = subject_patterns[subj2][condition]['frequency'][band_name]\n","\n","                pattern1_flat = pattern1.flatten()\n","                pattern2_flat = pattern2.flatten()\n","\n","                corr = np.corrcoef(pattern1_flat, pattern2_flat)[0, 1]\n","                pairwise_correlations.append(corr)\n","\n","            if pairwise_correlations:\n","                results['frequency'][band_name][condition] = {\n","                    'correlations': np.array(pairwise_correlations),\n","                    'mean': np.mean(pairwise_correlations),\n","                    'std': np.std(pairwise_correlations),\n","                    'median': np.median(pairwise_correlations)\n","                }\n","\n","    return results\n","\n","# ============================================================================\n","# VISUALIZATION FUNCTIONS\n","# ============================================================================\n","\n","def plot_q3_results(q3_results, save_path, task):\n","    \"\"\"Visualize Q3: channel consistency across subjects.\"\"\"\n","    conditions = list(q3_results['temporal'].keys())\n","    n_conditions = len(conditions)\n","\n","    # Create figure with appropriate size\n","    n_rows = min(3, (n_conditions + 1) // 2 + 1)\n","    fig, axes = plt.subplots(n_rows, 3, figsize=(18, 6*n_rows))\n","    axes = axes.flatten()\n","\n","    # Plot temporal consistency per channel for each condition\n","    for idx, condition in enumerate(conditions[:6]):  # Limit to 6 conditions for space\n","        if idx >= len(axes) - 3:\n","            break\n","\n","        ax = axes[idx]\n","        scores = q3_results['temporal'][condition]['channel_scores']\n","        channel_names = q3_results['temporal'][condition]['channel_names']\n","\n","        x = np.arange(len(scores))\n","        bars = ax.bar(x, scores, alpha=0.7, edgecolor='black', linewidth=1)\n","\n","        # Color code by performance\n","        colors = ['green' if s > 0.7 else 'orange' if s > 0.5 else 'red' for s in scores]\n","        for bar, color in zip(bars, colors):\n","            bar.set_color(color)\n","\n","        ax.axhline(0.7, color='green', linestyle='--', alpha=0.5, linewidth=2, label='High')\n","        ax.axhline(0.5, color='orange', linestyle='--', alpha=0.5, linewidth=2, label='Medium')\n","        ax.set_xlabel('Channel', fontsize=10)\n","        ax.set_ylabel('Mean Correlation', fontsize=10)\n","        ax.set_title(f'Condition {condition} (Temporal)', fontweight='bold', fontsize=11)\n","        ax.set_xticks(x[::max(1, len(x)//10)])\n","        ax.set_xticklabels([channel_names[i] for i in x[::max(1, len(x)//10)]], rotation=45, ha='right')\n","        ax.set_ylim([0, 1])\n","        ax.grid(True, alpha=0.3, axis='y')\n","        if idx == 0:\n","            ax.legend(fontsize=8)\n","\n","    # Frequency band comparison\n","    n_conditions_shown = min(len(conditions), 6)  # Only show up to 6 conditions\n","    if len(axes) > n_conditions_shown:\n","        ax = axes[n_conditions_shown]  # Use the next available subplot\n","        band_names = list(freq_bands.keys())\n","\n","        # Show frequency bands for first 4 displayed conditions\n","        for condition in conditions[:4]:\n","            if condition in q3_results['frequency'][band_names[0]]:  # Check if data exists\n","                band_scores = [q3_results['frequency'][band][condition]['mean']\n","                              for band in band_names if condition in q3_results['frequency'][band]]\n","                if len(band_scores) == len(band_names):  # Only plot if we have all bands\n","                    ax.plot(band_names, band_scores, marker='o', label=f'C{condition}', linewidth=2, markersize=8)\n","\n","        ax.axhline(0.7, color='green', linestyle='--', alpha=0.3, linewidth=2)\n","        ax.axhline(0.5, color='orange', linestyle='--', alpha=0.3, linewidth=2)\n","        ax.set_xlabel('Frequency Band', fontsize=10)\n","        ax.set_ylabel('Mean Correlation', fontsize=10)\n","        ax.set_title('Frequency Band Consistency', fontweight='bold', fontsize=11)\n","        ax.legend(fontsize=8)\n","        ax.grid(True, alpha=0.3)\n","        ax.set_ylim([0, 1])\n","\n","    # Statistical summary\n","    if len(axes) > n_conditions_shown + 1:\n","        ax = axes[n_conditions_shown + 1]\n","        ax.axis('off')\n","\n","        summary_text = \"STATISTICAL SUMMARY\\n\" + \"=\"*40 + \"\\n\\n\"\n","        summary_text += \"Temporal Domain:\\n\"\n","        for condition in conditions[:8]:  # Limit display\n","            mean = q3_results['temporal'][condition]['mean']\n","            std = q3_results['temporal'][condition]['std']\n","            summary_text += f\"  C{condition}: μ={mean:.3f}, σ={std:.3f}\\n\"\n","\n","        ax.text(0.1, 0.5, summary_text, fontsize=9, family='monospace',\n","                verticalalignment='center', transform=ax.transAxes)\n","\n","    # Hide unused axes\n","    for idx in range(n_conditions_shown + 2, len(axes)):\n","        axes[idx].axis('off')\n","\n","    plt.suptitle(f'Q3: Channel Consistency Across Subjects - {task} Task',\n","                 fontsize=16, fontweight='bold')\n","    plt.tight_layout()\n","    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","    plt.close()\n","    print(f\"Saved Q3 plot: {save_path}\")\n","\n","\n","def plot_q4_results(q4_results, save_path, task):\n","    \"\"\"Visualize Q4: label consistency across subjects.\"\"\"\n","    conditions = list(q4_results['temporal'].keys())\n","    band_names = list(freq_bands.keys())\n","\n","    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n","\n","    # Plot 1: Temporal consistency per condition\n","    ax = axes[0, 0]\n","    means = [q4_results['temporal'][c]['mean'] for c in conditions]\n","    stds = [q4_results['temporal'][c]['std'] for c in conditions]\n","\n","    x = np.arange(len(conditions))\n","    bars = ax.bar(x, means, yerr=stds, capsize=5, alpha=0.7, edgecolor='black', linewidth=1)\n","\n","    colors = ['green' if m > 0.6 else 'orange' if m > 0.3 else 'red' for m in means]\n","    for bar, color in zip(bars, colors):\n","        bar.set_color(color)\n","\n","    ax.axhline(0.6, color='green', linestyle='--', alpha=0.3, label='High')\n","    ax.axhline(0.3, color='orange', linestyle='--', alpha=0.3, label='Medium')\n","    ax.set_xlabel('Condition', fontsize=10)\n","    ax.set_ylabel('Mean Correlation', fontsize=10)\n","    ax.set_title('Temporal Label Consistency', fontweight='bold', fontsize=11)\n","    ax.set_xticks(x)\n","    ax.set_xticklabels([f'C{c}' for c in conditions])\n","    ax.legend(fontsize=8)\n","    ax.grid(True, alpha=0.3, axis='y')\n","    ax.set_ylim([0, 1])\n","\n","    # Plot 2: Distribution of correlations\n","    ax = axes[0, 1]\n","    all_corrs = [q4_results['temporal'][c]['correlations'] for c in conditions]\n","    bp = ax.boxplot(all_corrs, labels=[f'C{c}' for c in conditions], patch_artist=True)\n","    for patch in bp['boxes']:\n","        patch.set_facecolor('lightblue')\n","    ax.axhline(0.6, color='green', linestyle='--', alpha=0.3)\n","    ax.axhline(0.3, color='orange', linestyle='--', alpha=0.3)\n","    ax.set_xlabel('Condition', fontsize=10)\n","    ax.set_ylabel('Correlation', fontsize=10)\n","    ax.set_title('Distribution of Pairwise Correlations', fontweight='bold', fontsize=11)\n","    ax.grid(True, alpha=0.3, axis='y')\n","\n","    # Plot 3: Mean vs Median\n","    ax = axes[0, 2]\n","    means = [q4_results['temporal'][c]['mean'] for c in conditions]\n","    medians = [q4_results['temporal'][c]['median'] for c in conditions]\n","\n","    x = np.arange(len(conditions))\n","    width = 0.35\n","    ax.bar(x - width/2, means, width, label='Mean', alpha=0.7)\n","    ax.bar(x + width/2, medians, width, label='Median', alpha=0.7)\n","    ax.axhline(0.6, color='green', linestyle='--', alpha=0.3)\n","    ax.axhline(0.3, color='orange', linestyle='--', alpha=0.3)\n","    ax.set_xticks(x)\n","    ax.set_xticklabels([f'C{c}' for c in conditions])\n","    ax.set_ylabel('Correlation', fontsize=10)\n","    ax.set_title('Mean vs Median Consistency', fontweight='bold', fontsize=11)\n","    ax.legend()\n","    ax.grid(True, alpha=0.3, axis='y')\n","\n","    # Plot 4: Frequency band comparison\n","    ax = axes[1, 0]\n","    x = np.arange(len(band_names))\n","    width = 0.8 / len(conditions)\n","\n","    for i, condition in enumerate(conditions[:8]):  # Limit to 8 for visibility\n","        if condition not in q4_results['frequency'][band_names[0]]:\n","            continue\n","        band_scores = [q4_results['frequency'][band][condition]['mean']\n","                      for band in band_names]\n","        ax.bar(x + i*width, band_scores, width, label=f'C{condition}', alpha=0.7)\n","\n","    ax.axhline(0.6, color='green', linestyle='--', alpha=0.3)\n","    ax.axhline(0.3, color='orange', linestyle='--', alpha=0.3)\n","    ax.set_xlabel('Frequency Band', fontsize=10)\n","    ax.set_ylabel('Mean Correlation', fontsize=10)\n","    ax.set_title('Frequency Band Label Consistency', fontweight='bold', fontsize=11)\n","    ax.set_xticks(x + width * (len(conditions[:8])-1) / 2)\n","    ax.set_xticklabels(band_names)\n","    ax.legend(fontsize=8, ncol=2)\n","    ax.grid(True, alpha=0.3, axis='y')\n","\n","    # Plot 5: Temporal vs best frequency band\n","    ax = axes[1, 1]\n","    temporal_means = [q4_results['temporal'][c]['mean'] for c in conditions]\n","\n","    # Find best band for each condition\n","    best_freq_means = []\n","    for condition in conditions:\n","        band_scores = {band: q4_results['frequency'][band][condition]['mean']\n","                      for band in band_names if condition in q4_results['frequency'][band]}\n","        if band_scores:\n","            best_freq_means.append(max(band_scores.values()))\n","        else:\n","            best_freq_means.append(0)\n","\n","    x = np.arange(len(conditions))\n","    width = 0.35\n","    ax.bar(x - width/2, temporal_means, width, label='Temporal', alpha=0.7)\n","    ax.bar(x + width/2, best_freq_means, width, label='Best Frequency', alpha=0.7)\n","    ax.axhline(0.6, color='green', linestyle='--', alpha=0.3)\n","    ax.axhline(0.3, color='orange', linestyle='--', alpha=0.3)\n","    ax.set_xticks(x)\n","    ax.set_xticklabels([f'C{c}' for c in conditions])\n","    ax.set_ylabel('Mean Correlation', fontsize=10)\n","    ax.set_title('Temporal vs Frequency Consistency', fontweight='bold', fontsize=11)\n","    ax.legend()\n","    ax.grid(True, alpha=0.3, axis='y')\n","\n","    # Plot 6: Statistical summary\n","    ax = axes[1, 2]\n","    ax.axis('off')\n","\n","    summary_text = \"STATISTICAL SUMMARY\\n\" + \"=\"*40 + \"\\n\\n\"\n","    summary_text += \"Temporal Domain:\\n\"\n","    for condition in conditions[:8]:  # Limit display\n","        mean = q4_results['temporal'][condition]['mean']\n","        median = q4_results['temporal'][condition]['median']\n","        summary_text += f\"  C{condition}: μ={mean:.3f}, med={median:.3f}\\n\"\n","\n","    summary_text += \"\\nBest Frequency Band:\\n\"\n","    for condition in conditions[:8]:\n","        band_scores = {band: q4_results['frequency'][band][condition]['mean']\n","                      for band in band_names if condition in q4_results['frequency'][band]}\n","        if band_scores:\n","            best_band = max(band_scores, key=band_scores.get)\n","            best_score = band_scores[best_band]\n","            summary_text += f\"  C{condition}: {best_band} ({best_score:.3f})\\n\"\n","\n","    ax.text(0.1, 0.5, summary_text, fontsize=9, family='monospace',\n","            verticalalignment='center', transform=ax.transAxes)\n","\n","    plt.suptitle(f'Q4: Label Consistency Across Subjects - {task} Task',\n","                 fontsize=16, fontweight='bold')\n","    plt.tight_layout()\n","    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n","    plt.close()\n","    print(f\"Saved Q4 plot: {save_path}\")\n","\n","# ============================================================================\n","# REPORTING FUNCTIONS\n","# ============================================================================\n","\n","def create_summary_report(q3_results, q4_results, task):\n","    \"\"\"Generate comprehensive text report.\"\"\"\n","    print(\"\\n\" + \"=\"*80)\n","    print(f\"CROSS-SUBJECT CONSISTENCY ANALYSIS SUMMARY - {task} TASK\")\n","    print(\"=\"*80)\n","\n","    conditions = list(q3_results['temporal'].keys())\n","\n","    # Q3 Summary\n","    print(\"\\n\" + \"-\"*80)\n","    print(\"Q3: CHANNEL CONSISTENCY ACROSS SUBJECTS\")\n","    print(\"-\"*80)\n","    print(\"\\nDoes the same channel for the same label look similar across subjects?\")\n","    print(\"\\nTemporal Domain (per condition):\")\n","\n","    for condition in conditions:\n","        mean = q3_results['temporal'][condition]['mean']\n","        std = q3_results['temporal'][condition]['std']\n","        print(f\"  Condition {condition:3d}: {mean:.3f} ± {std:.3f}\")\n","\n","    overall_temporal_mean = np.mean([q3_results['temporal'][c]['mean'] for c in conditions])\n","    print(f\"\\n  Overall:         {overall_temporal_mean:.3f}\")\n","\n","    print(\"\\nFrequency Domain (averaged across conditions):\")\n","    for band in freq_bands.keys():\n","        band_means = [q3_results['frequency'][band][c]['mean']\n","                     for c in conditions if c in q3_results['frequency'][band]]\n","        if band_means:\n","            print(f\"  {band:8s}: {np.mean(band_means):.3f}\")\n","\n","    # Q4 Summary\n","    print(\"\\n\" + \"-\"*80)\n","    print(\"Q4: LABEL CONSISTENCY ACROSS SUBJECTS\")\n","    print(\"-\"*80)\n","    print(\"\\nAre label representations consistent across subjects?\")\n","    print(\"\\nTemporal Domain (per condition):\")\n","\n","    for condition in conditions:\n","        if condition in q4_results['temporal']:\n","            mean = q4_results['temporal'][condition]['mean']\n","            median = q4_results['temporal'][condition]['median']\n","            std = q4_results['temporal'][condition]['std']\n","            print(f\"  Condition {condition:3d}: μ={mean:.3f}, med={median:.3f}, σ={std:.3f}\")\n","\n","    valid_conditions = [c for c in conditions if c in q4_results['temporal']]\n","    if valid_conditions:\n","        overall_q4_temporal = np.mean([q4_results['temporal'][c]['mean'] for c in valid_conditions])\n","        print(f\"\\n  Overall:         {overall_q4_temporal:.3f}\")\n","\n","    print(\"\\nFrequency Domain (best band per condition):\")\n","    for condition in conditions:\n","        band_scores = {band: q4_results['frequency'][band][condition]['mean']\n","                      for band in freq_bands.keys()\n","                      if condition in q4_results['frequency'][band]}\n","        if band_scores:\n","            best_band = max(band_scores, key=band_scores.get)\n","            best_score = band_scores[best_band]\n","            print(f\"  Condition {condition:3d}: {best_band} ({best_score:.3f})\")\n","\n","\n","def save_results_to_csv(q3_results, q4_results, output_dir):\n","    \"\"\"Save results to CSV files.\"\"\"\n","    conditions = list(q3_results['temporal'].keys())\n","\n","    # Q3 CSV - Channel consistency\n","    q3_rows = []\n","    for condition in conditions:\n","        channel_names = q3_results['temporal'][condition]['channel_names']\n","        channel_scores = q3_results['temporal'][condition]['channel_scores']\n","        for ch_name, score in zip(channel_names, channel_scores):\n","            q3_rows.append({\n","                'condition': condition,\n","                'channel': ch_name,\n","                'temporal_correlation': score\n","            })\n","\n","    if q3_rows:\n","        q3_df = pd.DataFrame(q3_rows)\n","        q3_df.to_csv(output_dir / 'q3_channel_consistency.csv', index=False)\n","        print(f\"\\nSaved Q3 results to: {output_dir / 'q3_channel_consistency.csv'}\")\n","\n","    # Q4 CSV - Label consistency\n","    q4_rows = []\n","    for condition in conditions:\n","        if condition not in q4_results['temporal']:\n","            continue\n","\n","        row = {\n","            'condition': condition,\n","            'temporal_mean': q4_results['temporal'][condition]['mean'],\n","            'temporal_std': q4_results['temporal'][condition]['std'],\n","            'temporal_median': q4_results['temporal'][condition]['median']\n","        }\n","\n","        # Add frequency bands\n","        for band in freq_bands.keys():\n","            if condition in q4_results['frequency'][band]:\n","                row[f'{band}_mean'] = q4_results['frequency'][band][condition]['mean']\n","                row[f'{band}_std'] = q4_results['frequency'][band][condition]['std']\n","\n","        q4_rows.append(row)\n","\n","    if q4_rows:\n","        q4_df = pd.DataFrame(q4_rows)\n","        q4_df.to_csv(output_dir / 'q4_label_consistency.csv', index=False)\n","        print(f\"Saved Q4 results to: {output_dir / 'q4_label_consistency.csv'}\")\n","\n","# ============================================================================\n","# MAIN EXECUTION\n","# ============================================================================\n","\n","if __name__ == \"__main__\":\n","    print(f\"\\n{'='*80}\")\n","    print(f\"Cross-Subject Consistency Analysis - {TASK} Task\")\n","    print(f\"{'='*80}\\n\")\n","\n","    # Get task configuration\n","    sfreq = TASK_CONFIG[TASK]['sfreq']\n","\n","    # Load ALL data for all subjects\n","    root_dir = DATA_PATHS[TASK]\n","    print(f\"Loading ALL {TASK} data from: {root_dir}\")\n","\n","    kwargs = {}\n","    if TASK == 'MI':\n","        kwargs['label_dir'] = None  # Set your label directory if needed\n","\n","    subject_data = load_all_subjects_for_task(TASK, root_dir, **kwargs)\n","\n","    print(f\"\\n{'='*40}\")\n","    print(f\"Found {len(subject_data)} subjects\")\n","    for sid, conds in subject_data.items():\n","        print(f\"  Subject {sid}: {len(conds)} conditions, \", end='')\n","        total_trials = sum(trials.shape[0] for trials in conds.values())\n","        print(f\"{total_trials} total trials\")\n","    print(f\"{'='*40}\\n\")\n","\n","    # # Analyze Q3: Channel consistency\n","    # print(\"\\n\" + \"=\"*80)\n","    # print(\"STARTING Q3 ANALYSIS\")\n","    # print(\"=\"*80)\n","    # q3_results = analyze_q3_channel_consistency(subject_data, TASK, sfreq)\n","    # plot_q3_results(q3_results, OUTPUT_DIR / 'q3_channel_consistency.png', TASK)\n","\n","    # Analyze Q4: Label consistency\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"STARTING Q4 ANALYSIS\")\n","    print(\"=\"*80)\n","    q4_results = analyze_q4_label_consistency(subject_data, TASK, sfreq)\n","    plot_q4_results(q4_results, OUTPUT_DIR / 'q4_label_consistency.png', TASK)\n","\n","    # Generate report\n","    create_summary_report(q3_results, q4_results, TASK)\n","\n","    # Save results to CSV\n","    save_results_to_csv(q3_results, q4_results, OUTPUT_DIR)\n","\n","    print(f\"\\n{'='*80}\")\n","    print(f\"Analysis complete! Results saved to: {OUTPUT_DIR}\")\n","    print(f\"{'='*80}\\n\")"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1LjHAixfo6rUhJN2Ssy_XPUy5Qbr4wWvV","authorship_tag":"ABX9TyNRnRpcvgSkWm5op8DVbxBR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}